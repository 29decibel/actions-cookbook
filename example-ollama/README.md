# Ollama example

This shows the way to connect to local Ollama
and chat with Mistral.

You need to have [Ollama](https://ollama.ai/) running and [mistral](https://ollama.ai/library/mistral) model.


To run this use `action-server start`.

You may then check http://localhost:8080 for the Action Server UI.
There you can try the action `chat` out.

You can expose this action to internet with `action-server start --expose`.
 